{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from bert_score import score\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773fca15f08a42d5928bed210b7cd4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/csgrad/sunilruf/'\n",
    "path = \"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/rank_64/rank_64_2/checkpoint-50\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = '../../../ExTES.json'  # Change 'output.json' to your desired file name\n",
    "\n",
    "# Write the list to the JSON file\n",
    "with open(json_file_path, 'rb') as json_file:\n",
    "    extes_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11178"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene': 'Body Image Concerns and Eating Disorders',\n",
       " 'description': \"I've recently gained weight and I'm really struggling with accepting my body. I have a trip to the beach coming up and I'm feeling anxious and self-conscious about wearing a swimsuit in front of my friends.\",\n",
       " 'content': [{'User': \"I don't know what to do. I've tried dieting and exercising, but nothing seems to be working.\"},\n",
       "  {'AI Strategy': '',\n",
       "   'AI': \"It sounds like you're going through a tough time. Can you tell me more about your feelings and concerns about your body image?\"},\n",
       "  {'User': \"I just feel like I'm not attractive anymore. None of my clothes fit properly and I keep comparing myself to others.\"},\n",
       "  {'AI Strategy': '',\n",
       "   'AI': \"I can understand how difficult this must be for you. It's common to have these feelings, but it's important to remember that beauty comes in all shapes and sizes. Have you considered talking to a therapist or counselor about your body image concerns?\"},\n",
       "  {'User': \"I've thought about it, but I'm not sure if it would really help.\"},\n",
       "  {'User': 'I just want to feel comfortable and confident in my own skin.'},\n",
       "  {'AI Strategy': '',\n",
       "   'AI': \"Seeking professional help can provide you with the support and guidance you need. They can help you develop a healthier relationship with your body and work through any negative thoughts or emotions. It might be worth giving it a try. Have you spoken to your friends or loved ones about how you're feeling?\"},\n",
       "  {'User': \"I haven't really talked to anyone about it. I'm afraid they won't understand or think I'm just being dramatic.\"},\n",
       "  {'AI Strategy': '',\n",
       "   'AI': 'Opening up to someone you trust can be therapeutic. They may have experienced similar feelings and can offer you support. Remember, true friends and loved ones will listen without judgment and be there for you. It might be worth sharing your struggles with them.'},\n",
       "  {'User': \"I guess you're right. Maybe it's time for me to have that conversation with my best friend. I just need someone to listen and understand.\"},\n",
       "  {'AI Strategy': '',\n",
       "   'AI': \"That's a great step in the right direction. Having someone to talk to can make a big difference. Additionally, practicing self-care activities like doing things that make you happy or engaging in relaxation techniques can help improve your overall well-being and self-esteem. Is there something you enjoy doing that you could incorporate into your routine? Maybe a hobby or activity that boosts your mood?\"},\n",
       "  {'User': \"I used to love painting, but I haven't done it in a while. Maybe I should start again. It could help distract me from these negative thoughts.\"},\n",
       "  {'AI Strategy': '',\n",
       "   'AI': 'That sounds like a wonderful idea! Engaging in activities that bring you joy can be a great form of self-care and can also help improve your mood. Painting can be a therapeutic outlet for your emotions. Give it a try and see how it makes you feel. Remember to be gentle with yourself and celebrate small victories along the way. You deserve to feel happy and confident.'},\n",
       "  {'User': 'Thank you, I really appreciate your advice. It gives me hope that things can get better.'},\n",
       "  {'AI Strategy': '',\n",
       "   'AI': \"You're welcome! Remember, you're not alone in this journey, and there is support available to you. If you ever need someone to talk to, don't hesitate to reach out. You're stronger than you think and I believe in you. Take care of yourself and keep taking small steps towards self-love and acceptance.\"}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extes_data[94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'User' in extes_data[494]['content'][0]:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/csgrad/sunilruf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(tokenizer, messages):\n",
    "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "    outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "    )\n",
    "    candidate = (tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split(\"[/INST]\")[-1])\n",
    "    \n",
    "    return candidate, input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "for i in range(50):\n",
    "    #num = random.randint(0, 11170)\n",
    "    num = i\n",
    "    data = extes_data[num]['content']\n",
    "    print(num)\n",
    "    messages = []\n",
    "    bleu_1 = []\n",
    "    rouge_1 = []\n",
    "    rouge_2 = []\n",
    "    rougle_l = []\n",
    "    Bert_P = []\n",
    "    Bert_R = []\n",
    "    Bert_F1 = []\n",
    "    for i in range(0, len(data)-1, 2):\n",
    "        #print(data[i]['User'])\n",
    "        if 'User' not in data[i]:\n",
    "            i = i+1\n",
    "        messages.append({\"role\": \"user\", \"content\":  data[i]['User']})\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(messages, tokenize=True, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "        )\n",
    "        candidate = (tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split(\"[/INST]\")[-1])\n",
    "        reference = data[i+1]['AI']\n",
    "        \n",
    "        references = reference.split(\" \")\n",
    "        candidates = candidate.split(\" \")\n",
    "        print(\"Reference: \", reference)\n",
    "        print(\"Candidate: \", candidate)\n",
    "        smoothie = SmoothingFunction().method4\n",
    "\n",
    "        bleu_1.append(sentence_bleu(references, candidates, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n",
    "        results = rouge.compute(predictions=[candidate], references=[reference])\n",
    "        rouge_1.append(results['rouge1'])   \n",
    "        rouge_2.append(results['rouge2'])\n",
    "        rougle_l.append(results['rougeL'])\n",
    "        \n",
    "        P, R, F1 = score([candidate], [reference], lang='en', model_type='bert-base-uncased')\n",
    "        Bert_P.append(P)\n",
    "        Bert_R.append(R)\n",
    "        Bert_F1.append(F1)\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": reference})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu1 average:  0.014285714285714289\n",
      "Rouge1 average:  0.33006797281087563\n",
      "Rouge2 average:  0.09999999999999999\n",
      "RougeL average:  0.2526989204318273\n",
      "Bert_P average:  tensor([0.5686])\n",
      "Bert_R average:  tensor([0.5642])\n",
      "Bert_F1 average:  tensor([0.5661])\n",
      "Bleu1 max:  0.028571428571428577\n",
      "Rouge1 max:  0.4634146341463415\n",
      "Rouge2 max:  0.19999999999999998\n",
      "RougeL max:  0.34146341463414637\n",
      "Bert_P max:  tensor([0.6851])\n",
      "Bert_R max:  tensor([0.6523])\n",
      "Bert_F1 max:  tensor([0.6683])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"Bleu1 average: \", sum(bleu_1)/len(bleu_1))\n",
    "print(\"Rouge1 average: \", sum(rouge_1)/len(rouge_1))\n",
    "print(\"Rouge2 average: \", sum(rouge_2)/len(rouge_2))\n",
    "print(\"RougeL average: \", sum(rougle_l)/len(rougle_l))\n",
    "print(\"Bert_P average: \", sum(Bert_P)/len(Bert_P))\n",
    "print(\"Bert_R average: \", sum(Bert_R)/len(Bert_R))\n",
    "print(\"Bert_F1 average: \", sum(Bert_F1)/len(Bert_F1))\n",
    "\n",
    "print(\"Bleu1 max: \", max(bleu_1))\n",
    "print(\"Rouge1 max: \", max(rouge_1))\n",
    "print(\"Rouge2 max: \", max(rouge_2))\n",
    "print(\"RougeL max: \", max(rougle_l))\n",
    "print(\"Bert_P max: \", max(Bert_P))\n",
    "print(\"Bert_R max: \", max(Bert_R))\n",
    "print(\"Bert_F1 max: \", max(Bert_F1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem Take care yourself do hesitate ask help needed Have day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/csgrad/sunilruf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def extract_head_nouns_verbs(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Perform part-of-speech tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Extract head nouns and verbs\n",
    "    head_nouns_verbs = [word for word, tag in tagged_words if tag.startswith('NN') or tag.startswith('VB')]\n",
    "    \n",
    "    return head_nouns_verbs\n",
    "\n",
    "# Example usage\n",
    "#text = \"I went to the park and played with my dog.\"\n",
    "text = \"No problem at all! Take care of yourself and don't hesitate to ask for help when needed. Have a fantastic day!\"\n",
    "head_nouns_verbs = extract_head_nouns_verbs(text)\n",
    "print(\" \".join(head_nouns_verbs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def recursive_summary(text, model, tokenizer, max_length=512, threshold=100):\n",
    "    inputs = tokenizer.encode(text, return_tensors='pt', max_length=max_length, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if len(summary) <= threshold:\n",
    "        return summary\n",
    "    else:\n",
    "        return recursive_summary(summary, model, tokenizer, max_length, threshold)\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a long piece of text that needs to be summarized recursively. It contains multiple paragraphs and sentences. The goal is to generate a concise summary of the entire text.\"\n",
    "summary = recursive_summary(text, model, tokenizer)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.1/920.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy) (66.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/csgrad/sunilruf/.local/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.12.3\n",
      "    Uninstalling typer-0.12.3:\n",
      "      Successfully uninstalled typer-0.12.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 4.28.3 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.0 murmurhash-1.0.10 preshed-3.0.9 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (66.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/csgrad/sunilruf/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_head_nouns(text):\n",
    "    # Parse the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract head nouns\n",
    "    head_nouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\" and token.head.pos_ != \"NOUN\":\n",
    "            head_nouns.append(token.text)\n",
    "    \n",
    "    return head_nouns\n",
    "\n",
    "# Example usage\n",
    "user_response = \"I am very sad\"\n",
    "head_nouns = extract_head_nouns(user_response)\n",
    "print(head_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence stop words, the, is, with.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/csgrad/sunilruf/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "text = \"This is a sample sentence with some stop words, such as the, is, and with.\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
