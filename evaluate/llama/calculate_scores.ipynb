{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/csgrad/sunilruf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from bert_score import score\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import evaluate\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from tqdm import tqdm\n",
    "smoothie = SmoothingFunction().method4\n",
    "rouge = evaluate.load('rouge')\n",
    "messages_final = json.load(open('data/messages_final.json'))\n",
    "messages_boh_original_final = json.load(open('data/messages_boh_original_final.json'))\n",
    "messages_no_sw_original_final = json.load(open('data/messages_no_sw_original_final.json'))\n",
    "test_data = json.load(open('data/test_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/csgrad/sunilruf/'\n",
    "path = \"/data/sunilruf/llama/fp16_no/rank_32_1/checkpoint-50/\"\n",
    "torch.cuda.empty_cache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:21<36:03, 21.86s/it]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "  8%|▊         | 8/100 [01:38<16:49, 10.97s/it]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      " 14%|█▍        | 14/100 [02:53<17:53, 12.48s/it]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      " 53%|█████▎    | 53/100 [10:52<09:34, 12.23s/it]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      " 55%|█████▌    | 55/100 [11:17<09:20, 12.46s/it]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      " 58%|█████▊    | 58/100 [11:48<07:46, 11.12s/it]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      " 66%|██████▌   | 66/100 [13:19<06:20, 11.19s/it]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "100%|██████████| 100/100 [20:20<00:00, 12.21s/it]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "bleu_2_final, bleu_2_boh_original_final, bleu_2_no_sw_original_final = [], [], []\n",
    "bleu_4_final, bleu_4_boh_original_final, bleu_4_no_sw_original_final = [], [], []\n",
    "bert_score_final, bert_score_boh_original_final, bert_score_no_sw_original_final = [], [], []\n",
    "rouge_final, rouge_boh_original_final, rouge_no_sw_original_final = [], [], []\n",
    "meteor_final, meteor_boh_original_final, meteor_no_sw_original_final = [], [], []\n",
    "\n",
    "for j in tqdm(range(100)):\n",
    "    \n",
    "    for i in range(1, len(test_data[j]),2):\n",
    "        #print(i)\n",
    "        if messages_final[j][i]['content'] == '' or test_data[j][i]['content'] == '' or messages_boh_original_final[j][i]['content'] == '' or messages_no_sw_original_final[j][i]['content'] == '':\n",
    "            continue\n",
    "        #print(messages_final[j][i]['content'])\n",
    "        \n",
    "        bleu_2_final.append(sentence_bleu(messages_final[j][i]['content'], test_data[j][i]['content'], weights=(0.5,0.5), smoothing_function=smoothie))\n",
    "        bleu_2_boh_original_final.append(sentence_bleu(messages_boh_original_final[j][i]['content'], test_data[j][i]['content'], weights=(0.5,0.5), smoothing_function=smoothie))\n",
    "        bleu_2_no_sw_original_final.append(sentence_bleu(messages_no_sw_original_final[j][i]['content'], test_data[j][i]['content'], weights=(0.5,0.5), smoothing_function=smoothie))\n",
    "        bleu_4_final.append(sentence_bleu(messages_final[j][i]['content'], test_data[j][i]['content'], weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie))\n",
    "        bleu_4_boh_original_final.append(sentence_bleu(messages_boh_original_final[j][i]['content'], test_data[j][i]['content'], weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie))\n",
    "        bleu_4_no_sw_original_final.append(sentence_bleu(messages_no_sw_original_final[j][i]['content'], test_data[j][i]['content'], weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie))\n",
    "        bert_score_final.append(score([messages_final[j][i]['content']], [test_data[j][i]['content']], lang='en', model_type='bert-base-uncased'))\n",
    "        bert_score_boh_original_final.append(score([messages_boh_original_final[j][i]['content']], [test_data[j][i]['content']], lang='en', model_type='bert-base-uncased'))\n",
    "        bert_score_no_sw_original_final.append(score([messages_no_sw_original_final[j][i]['content']], [test_data[j][i]['content']], lang='en', model_type='bert-base-uncased'))\n",
    "        rouge_final.append(rouge.compute(predictions=[messages_final[j][i]['content']], references=[test_data[j][i]['content']]))\n",
    "        rouge_boh_original_final.append(rouge.compute(predictions=[messages_boh_original_final[j][i]['content']], references=[test_data[j][i]['content']]))\n",
    "        rouge_no_sw_original_final.append(rouge.compute(predictions=[messages_no_sw_original_final[j][i]['content']], references=[test_data[j][i]['content']]))\n",
    "        meteor_final.append((nltk.translate.meteor_score.meteor_score([word_tokenize(messages_final[j][i]['content'])], word_tokenize(test_data[j][i]['content']))))\n",
    "        meteor_boh_original_final.append((nltk.translate.meteor_score.meteor_score([word_tokenize(messages_boh_original_final[j][i]['content'])], word_tokenize(test_data[j][i]['content']))))\n",
    "        meteor_no_sw_original_final.append((nltk.translate.meteor_score.meteor_score([word_tokenize(messages_no_sw_original_final[j][i]['content'])], word_tokenize(test_data[j][i]['content']))))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rougel_final = [i['rougeL'] for i in rouge_final]\n",
    "rougel_boh_original_final = [i['rougeL'] for i in rouge_boh_original_final]\n",
    "rougel_no_sw_original_final = [i['rougeL'] for i in rouge_no_sw_original_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_p_boh = [i[0].item() for i in bert_score_boh_original_final]\n",
    "bert_r_boh = [i[1].item() for i in bert_score_boh_original_final]\n",
    "bert_f1_boh = [i[2].item() for i in bert_score_boh_original_final]\n",
    "\n",
    "bert_p_no_sw = [i[0].item() for i in bert_score_no_sw_original_final]\n",
    "bert_r_no_sw = [i[1].item() for i in bert_score_no_sw_original_final]\n",
    "bert_f1_no_sw = [i[2].item() for i in bert_score_no_sw_original_final]\n",
    "\n",
    "bert_p = [i[0].item() for i in bert_score_final]\n",
    "bert_r = [i[1].item() for i in bert_score_final]\n",
    "bert_f1 = [i[2].item() for i in bert_score_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu 2 Final:  0.022428202955559518\n",
      "Bleu 2 Boh Original Final:  0.021479569843545387\n",
      "Bleu 2 No Sw Original Final:  0.021154077448443737\n",
      "Bleu 4 Final:  0.005236413654820756\n",
      "Bleu 4 Boh Original Final:  0.00512813465055343\n",
      "Bleu 4 No Sw Original Final:  0.005084864787179422\n",
      "Rouge L Final:  0.21916091051851186\n",
      "Rouge L Boh Original Final:  0.15947136262482073\n",
      "Rouge L No Sw Original Final:  0.13094655334062932\n",
      "Bert Precision Final:  0.5729715586562929\n",
      "Bert Recall Final:  0.5474313054122893\n",
      "Bert F1 Final:  0.5559400214085637\n",
      "Bert Precision Boh Original Final:  0.4919859349333333\n",
      "Bert Recall Boh Original Final:  0.4395153865037443\n",
      "Bert F1 Boh Original Final:  0.46017099744429796\n",
      "Bert Precision No Sw Original Final:  0.5137757430923025\n",
      "Bert Recall No Sw Original Final:  0.43972462767277165\n",
      "Bert F1 No Sw Original Final:  0.4688320349339219\n",
      "Meteor Final:  0.2794655716150985\n",
      "Meteor Boh Original Final:  0.20115607084670775\n",
      "Meteor No Sw Original Final:  0.21978682335195493\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"Bleu 2 Final: \", statistics.mean(bleu_2_final))\n",
    "print(\"Bleu 2 Boh Original Final: \", statistics.mean(bleu_2_boh_original_final))\n",
    "print(\"Bleu 2 No Sw Original Final: \", statistics.mean(bleu_2_no_sw_original_final))\n",
    "print(\"Bleu 4 Final: \", statistics.mean(bleu_4_final))\n",
    "print(\"Bleu 4 Boh Original Final: \", statistics.mean(bleu_4_boh_original_final))\n",
    "print(\"Bleu 4 No Sw Original Final: \", statistics.mean(bleu_4_no_sw_original_final))\n",
    "print(\"Rouge L Final: \", statistics.mean(rougel_final))\n",
    "print(\"Rouge L Boh Original Final: \", statistics.mean(rougel_boh_original_final))\n",
    "print(\"Rouge L No Sw Original Final: \", statistics.mean(rougel_no_sw_original_final))\n",
    "print(\"Bert Precision Final: \", statistics.mean(bert_p))\n",
    "print(\"Bert Recall Final: \", statistics.mean(bert_r))\n",
    "print(\"Bert F1 Final: \", statistics.mean(bert_f1))\n",
    "print(\"Bert Precision Boh Original Final: \", statistics.mean(bert_p_boh))\n",
    "print(\"Bert Recall Boh Original Final: \", statistics.mean(bert_r_boh))\n",
    "print(\"Bert F1 Boh Original Final: \", statistics.mean(bert_f1_boh))\n",
    "print(\"Bert Precision No Sw Original Final: \", statistics.mean(bert_p_no_sw))\n",
    "print(\"Bert Recall No Sw Original Final: \", statistics.mean(bert_r_no_sw))\n",
    "print(\"Bert F1 No Sw Original Final: \", statistics.mean(bert_f1_no_sw))\n",
    "print(\"Meteor Final: \", statistics.mean(meteor_final))\n",
    "print(\"Meteor Boh Original Final: \", statistics.mean(meteor_boh_original_final))\n",
    "print(\"Meteor No Sw Original Final: \", statistics.mean(meteor_no_sw_original_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "messages_boh = json.load(open('data/messages_boh_final.json'))\n",
    "messages_sw = json.load(open('data/messages_no_sw_final.json'))\n",
    "messages_final = json.load(open('data/messages_final.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 299.25it/s]\n"
     ]
    }
   ],
   "source": [
    "boh_length, sw_length, final_length = [], [], []\n",
    "for j in tqdm(range(100)):\n",
    "    for i in range(0,len(messages_final[j])):\n",
    "        final_length.append(len(tokenizer.tokenize(messages_final[j][i]['content'])))\n",
    "        sw_length.append(len(tokenizer.tokenize(messages_sw[j][i]['content'])))\n",
    "        boh_length.append(len(tokenizer.tokenize(messages_boh[j][i]['content'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual length  35.53556034482759\n",
      "boh length  17.744073275862068\n",
      "sw length  17.90301724137931\n"
     ]
    }
   ],
   "source": [
    "print(\"actual length \", statistics.mean(final_length))\n",
    "print(\"boh length \", statistics.mean(boh_length))\n",
    "print(\"sw length \", statistics.mean(sw_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "perplexity_final = torch.load('data/perplexity_final.pt')\n",
    "perplexity_boh = torch.load('data/perplexity_boh_final.pt')\n",
    "perplexity_sw = torch.load('data/perplexity_no_sw_final.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_final1 = []\n",
    "perplexity_final1.extend(per for sublist in perplexity_final for per in sublist )\n",
    "perplexity_boh1 = []\n",
    "perplexity_boh1.extend(per for sublist in perplexity_boh for per in sublist )\n",
    "perplexity_sw1 = []\n",
    "perplexity_sw1.extend(per for sublist in perplexity_sw for per in sublist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Final:  tensor(9.2122)\n",
      "Perplexity Boh Original Final:  tensor(53.3155)\n",
      "Perplexity No Sw Original Final:  tensor(34.3808)\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity Final: \", torch.mean(torch.tensor(perplexity_final1)))\n",
    "print(\"Perplexity Boh Original Final: \", torch.mean(torch.tensor(perplexity_boh1)))\n",
    "print(\"Perplexity No Sw Original Final: \", torch.mean(torch.tensor(perplexity_sw1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
