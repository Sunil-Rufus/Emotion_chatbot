{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0232c33684b74ff1be6917a5cacc002f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/experiment-6/checkpoint-50\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/experiment-6/checkpoint-50\", load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0d804df5594da5a8f2dd54ff7a6b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eab2c6620c642a5859ccb207aaf4d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/510 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aab4074ab76408abb0de88c8e9fa043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83820b5394c642a79f05e61dbcec1847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/865k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef72e0e66664292acf44eb97deff90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/881k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36c625a963840b7ae946f1d18f5b1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9460ead6164e76a574e25286cfc7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b742207e1ed44682bb24a2dffa9b16be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"thu-coai/esconv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 910\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 195\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 195\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(dataset['test'][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hai. My name is Tom and i really need your help.\n"
     ]
    }
   ],
   "source": [
    "print(data['dialog'][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(data['dialog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": data['dialog'][1]['text']\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity.compute(model_id=\"clm-gpu/checkpoint-138000\",\n",
    "                             add_start_token=False,\n",
    "                             predictions=input_texts, \n",
    "                             local_file_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 32000])\n",
      "tensor([[[-20.7714, -20.7558, -15.5265,  ..., -19.4121, -18.4629, -19.0976],\n",
      "         [-17.8636, -17.0862, -13.2464,  ..., -13.9007, -15.0784, -13.7542],\n",
      "         [-18.5422, -17.4162, -21.6535,  ..., -18.3918, -20.6574, -17.1051],\n",
      "         ...,\n",
      "         [-17.1520, -14.5641, -21.5261,  ..., -18.7692, -23.4860, -22.5856],\n",
      "         [-16.5943, -12.8658, -13.8658,  ..., -18.4012, -22.2466, -22.1705],\n",
      "         [-21.3237, -19.3820, -17.1049,  ..., -22.3432, -22.0288, -22.6460]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "1\n",
      "tensor(18.9341, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 32000])\n",
      "tensor([[[-20.8303, -20.8069, -15.5528,  ..., -19.4827, -18.5315, -19.1682],\n",
      "         [-17.7829, -16.9860, -13.2477,  ..., -13.9079, -15.0290, -13.8258],\n",
      "         [-18.6158, -17.4591, -21.6451,  ..., -18.6217, -20.8560, -17.3389],\n",
      "         ...,\n",
      "         [-14.8625, -12.6945, -12.4992,  ..., -16.4640, -22.5051, -20.9992],\n",
      "         [-16.9162, -13.5764, -10.6662,  ..., -19.6781, -22.1193, -21.4406],\n",
      "         [-16.8728, -14.1267,  -4.2380,  ..., -19.1267, -17.5007, -17.5330]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "2\n",
      "tensor(20.6048, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 113, 32000])\n",
      "tensor([[[-20.8165, -20.7931, -15.5432,  ..., -19.4649, -18.5177, -19.1563],\n",
      "         [-17.6921, -16.8601, -13.2136,  ..., -13.8347, -15.0476, -13.7605],\n",
      "         [-18.3280, -17.2196, -21.4452,  ..., -18.3065, -20.5155, -16.9305],\n",
      "         ...,\n",
      "         [-15.7645, -13.2527, -13.1902,  ..., -17.1629, -23.5418, -21.6102],\n",
      "         [-16.0348, -13.2321, -10.9391,  ..., -18.0407, -20.8473, -19.7272],\n",
      "         [-13.9694, -10.4138,  -0.2145,  ..., -16.7819, -18.1540, -16.3337]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "3\n",
      "tensor(19.8234, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 126, 32000])\n",
      "tensor([[[-20.8190, -20.7917, -15.5306,  ..., -19.4791, -18.5280, -19.1647],\n",
      "         [-17.7592, -16.9272, -13.1635,  ..., -13.8920, -15.0990, -13.8842],\n",
      "         [-18.5632, -17.3620, -21.4616,  ..., -18.5964, -20.8522, -17.3820],\n",
      "         ...,\n",
      "         [-17.1348, -14.7071, -14.2227,  ..., -18.4610, -24.4434, -23.0469],\n",
      "         [-17.2442, -14.0703, -10.5352,  ..., -19.3725, -21.7051, -20.9141],\n",
      "         [-15.0718, -11.6626,  -0.2407,  ..., -17.4966, -17.7661, -16.4653]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "4\n",
      "tensor(19.7078, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 32000])\n",
      "tensor([[[-20.8074, -20.7839, -15.5372,  ..., -19.4597, -18.5105, -19.1433],\n",
      "         [-17.7326, -16.9162, -13.2014,  ..., -13.8849, -15.0373, -13.7385],\n",
      "         [-18.4520, -17.3231, -21.4755,  ..., -18.3524, -20.6200, -17.0140],\n",
      "         ...,\n",
      "         [-16.3343, -13.8616, -11.2445,  ..., -17.4730, -24.2113, -22.6273],\n",
      "         [-17.3791, -15.0754, -11.0637,  ..., -19.0410, -20.4563, -19.7390],\n",
      "         [-16.1859, -12.6971,  -0.2752,  ..., -18.2811, -17.8641, -17.3104]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "5\n",
      "tensor(19.3313, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 173, 32000])\n",
      "tensor([[[-20.7728, -20.7455, -15.5163,  ..., -19.4291, -18.4799, -19.1185],\n",
      "         [-17.7272, -16.8951, -13.2447,  ..., -13.8463, -15.0514, -13.8189],\n",
      "         [-18.6235, -17.4448, -21.5747,  ..., -18.6333, -20.8637, -17.3877],\n",
      "         ...,\n",
      "         [-16.5297, -13.9868, -11.4477,  ..., -18.6821, -24.2778, -22.8344],\n",
      "         [-15.7400, -12.9353, -10.9861,  ..., -18.4724, -20.1887, -19.5700],\n",
      "         [-14.7533, -11.3302,  -0.2384,  ..., -15.5548, -17.1700, -16.6173]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "6\n",
      "tensor(19.0724, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 209, 32000])\n",
      "tensor([[[-20.7809, -20.7575, -15.5264,  ..., -19.4333, -18.4840, -19.1188],\n",
      "         [-17.7029, -16.8592, -13.2381,  ..., -13.8260, -15.0506, -13.8221],\n",
      "         [-18.5950, -17.4455, -21.4192,  ..., -18.5403, -20.8254, -17.2483],\n",
      "         ...,\n",
      "         [-18.2388, -16.0161, -12.4692,  ..., -19.1411, -25.1411, -24.0142],\n",
      "         [-18.2558, -16.0254, -13.3926,  ..., -20.1031, -21.8672, -20.5796],\n",
      "         [-16.5411, -13.1175,  -0.0492,  ..., -18.0209, -19.1253, -17.7440]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "7\n",
      "tensor(18.6492, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 32000])\n",
      "tensor([[[-20.8240, -20.8006, -15.5466,  ..., -19.4803, -18.5311, -19.1678],\n",
      "         [-17.7426, -16.9222, -13.1898,  ..., -13.8578, -15.0590, -13.8343],\n",
      "         [-18.5208, -17.3494, -21.5013,  ..., -18.5403, -20.7982, -17.3250],\n",
      "         ...,\n",
      "         [-16.7164, -14.3024, -12.4274,  ..., -18.2399, -23.7243, -22.4645],\n",
      "         [-17.3095, -14.5829, -11.9227,  ..., -19.6913, -21.0346, -20.3845],\n",
      "         [-16.7002, -13.3667,  -0.0406,  ..., -17.4273, -20.0699, -17.7925]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "8\n",
      "tensor(18.4338, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 243, 32000])\n",
      "tensor([[[-20.8165, -20.7930, -15.5330,  ..., -19.4727, -18.5235, -19.1602],\n",
      "         [-17.7666, -16.9346, -13.1397,  ..., -13.8838, -15.1026, -13.8545],\n",
      "         [-18.6347, -17.4267, -21.4648,  ..., -18.7167, -20.9921, -17.4760],\n",
      "         ...,\n",
      "         [-18.0240, -15.7623, -13.6217,  ..., -19.4459, -24.5905, -23.5885],\n",
      "         [-17.9603, -15.1478, -12.1907,  ..., -20.4683, -21.8050, -21.0865],\n",
      "         [-16.2934, -12.7876,  -0.0688,  ..., -17.7544, -20.1665, -17.7094]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "9\n",
      "tensor(18.1631, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 32000])\n",
      "tensor([[[-20.7945, -20.7672, -15.5212,  ..., -19.4547, -18.5055, -19.1402],\n",
      "         [-17.7225, -16.8788, -13.1561,  ..., -13.8749, -15.0565, -13.8358],\n",
      "         [-18.5133, -17.3498, -21.4743,  ..., -18.4938, -20.7672, -17.2877],\n",
      "         ...,\n",
      "         [-17.1436, -14.9971, -14.3760,  ..., -18.6338, -23.9170, -22.8018],\n",
      "         [-18.2966, -16.1266, -12.9782,  ..., -20.7324, -21.7995, -21.0956],\n",
      "         [-15.8625, -12.2988,  -0.1191,  ..., -17.6865, -19.8691, -17.9258]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "10\n",
      "tensor(17.8480, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 269, 32000])\n",
      "tensor([[[-20.8583, -20.8348, -15.5739,  ..., -19.5106, -18.5614, -19.1942],\n",
      "         [-17.8036, -17.0067, -13.2274,  ..., -13.8544, -15.1083, -13.8563],\n",
      "         [-18.6040, -17.4526, -21.6333,  ..., -18.6157, -20.7739, -17.3432],\n",
      "         ...,\n",
      "         [-18.1022, -15.7116, -12.1179,  ..., -19.7292, -24.1315, -23.1491],\n",
      "         [-17.7960, -15.1573, -13.2628,  ..., -20.8227, -21.8189, -20.9522],\n",
      "         [-15.5418, -12.0140,  -0.1253,  ..., -17.1800, -19.0823, -16.1195]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "11\n",
      "tensor(17.4676, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 280, 32000])\n",
      "tensor([[[-20.8231, -20.8036, -15.5411,  ..., -19.4794, -18.5282, -19.1669],\n",
      "         [-17.7807, -16.9565, -13.2163,  ..., -13.8803, -15.0932, -13.8725],\n",
      "         [-18.6858, -17.5154, -21.5433,  ..., -18.7210, -20.9729, -17.4900],\n",
      "         ...,\n",
      "         [-17.3430, -15.0559, -12.1809,  ..., -19.1633, -24.3293, -22.7434],\n",
      "         [-16.9887, -14.3715, -12.8715,  ..., -20.3838, -21.0966, -20.5036],\n",
      "         [-17.2837, -13.8276,  -0.0327,  ..., -19.2553, -19.7944, -18.3100]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "12\n",
      "tensor(17.1217, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 299, 32000])\n",
      "tensor([[[-20.8052, -20.7778, -15.5241,  ..., -19.4653, -18.5181, -19.1528],\n",
      "         [-17.7185, -16.8669, -13.1697,  ..., -13.8630, -15.1052, -13.8572],\n",
      "         [-18.5401, -17.3370, -21.3526,  ..., -18.6143, -20.8565, -17.3277],\n",
      "         ...,\n",
      "         [-18.2620, -15.9505, -10.1106,  ..., -19.5608, -24.7161, -23.3821],\n",
      "         [-17.6131, -15.2005,  -7.5442,  ..., -19.7649, -19.9856, -19.3074],\n",
      "         [-16.3516, -13.1944,  -0.0928,  ..., -17.4932, -18.2647, -17.2813]]],\n",
      "       device='cuda:1')\n",
      "torch.Size([])\n",
      "13\n",
      "tensor(16.9094, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 310, 32000])\n",
      "tensor([[[-2.0820e+01, -2.0792e+01, -1.5539e+01,  ..., -1.9480e+01,\n",
      "          -1.8527e+01, -1.9161e+01],\n",
      "         [-1.7757e+01, -1.6929e+01, -1.3248e+01,  ..., -1.3873e+01,\n",
      "          -1.5089e+01, -1.3849e+01],\n",
      "         [-1.8678e+01, -1.7511e+01, -2.1621e+01,  ..., -1.8691e+01,\n",
      "          -2.0949e+01, -1.7460e+01],\n",
      "         ...,\n",
      "         [-1.6764e+01, -1.3988e+01, -6.9804e+00,  ..., -1.8060e+01,\n",
      "          -2.2848e+01, -2.1211e+01],\n",
      "         [-1.7153e+01, -1.4231e+01, -6.0593e+00,  ..., -1.9694e+01,\n",
      "          -2.0158e+01, -1.9247e+01],\n",
      "         [-1.7389e+01, -1.4155e+01, -1.2033e-02,  ..., -1.9045e+01,\n",
      "          -1.9563e+01, -1.8481e+01]]], device='cuda:1')\n",
      "torch.Size([])\n",
      "14\n",
      "tensor(16.6619, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 1024\n",
    "stride = 128\n",
    "\n",
    "#encodings = tokenizer.encode_plus(data['dialog'][1]['text'], return_tensors=\"pt\").to('cuda')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "seq_len = 1024\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "messages = []\n",
    "i = 1\n",
    "while i<29:\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        messages.append({\"role\": \"user\", \"content\":  data['dialog'][i]['text']})\n",
    "        i+=2\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        #input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "        input_ids = tokenizer.apply_chat_template(messages, tokenize=True, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                labels=target_ids,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "        )\n",
    "            val = (tokenizer.decode(outputs[0], skip_special_tokens=True)[0].split(\"[/INST]\")[-1])\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            print(outputs.logits.shape)\n",
    "            print(torch.log(torch.softmax(outputs.logits,dim=-1)))\n",
    "            #decoded_output = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs.logits.argmax(-1)]\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            #print(outputs)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            print(neg_log_likelihood.shape)\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        print(len(nlls))\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    print(ppl)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(455.2411, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(333.4897, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(359.4648, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(452.8662, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(541.8233, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(641.3824, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(701.5912, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(781.6432, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(872.0746, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(965.7397, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1066.2126, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1179.7234, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1296.0780, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1420.3734, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 1024\n",
    "stride = 128\n",
    "\n",
    "#encodings = tokenizer.encode_plus(data['dialog'][1]['text'], return_tensors=\"pt\").to('cuda')\n",
    "tokenizer1.pad_token = tokenizer1.eos_token\n",
    "seq_len = 1024\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "messages = []\n",
    "i = 1\n",
    "while i<29:\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        messages.append({\"role\": \"user\", \"content\":  data['dialog'][i]['text']})\n",
    "        i+=2\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        #input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "        input_ids = tokenizer1.apply_chat_template(messages, tokenize=True, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = model1.generate(\n",
    "                input_ids=input_ids,\n",
    "                labels=target_ids,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "        )\n",
    "            val = (tokenizer1.decode(outputs[0], skip_special_tokens=True)[0].split(\"[/INST]\")[-1])\n",
    "            outputs = model1(input_ids, labels=target_ids)\n",
    "            #decoded_output = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs.logits.argmax(-1)]\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            #print(outputs)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    print(ppl)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "#max_length = model.config.n_positions\n",
    "stride = 512\n",
    "text = \"Absolutely, building this relationship means being there for each other through thick and thin. Whether it's sharing our joys, navigating challenges, or simply lending an ear, I'm dedicated to fostering a connection grounded in trust, empathy, and genuine care. Together, we can create a safe space where open dialogue and mutual support flourish.\"\n",
    "encodings = tokenizer.encode_plus(\"Of course! I'm here to help and chat with you whenever you need.\", return_tensors=\"pt\").to('cuda')\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6309, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.mean(torch.max(torch.softmax(outputs.logits, dim=-1), dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([[0.4461, 0.4940, 0.7285, 0.4279, 0.1381, 0.9768, 0.9906, 0.7321, 0.3723]],\n",
      "       device='cuda:1'),\n",
      "indices=tensor([[22478,   264, 10360,   354,   272,   266, 28127,  7768,    13]],\n",
      "       device='cuda:1'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.max(torch.softmax(outputs.logits, dim=-1),dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6380, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.mean(torch.max(torch.softmax(outputs.logits, dim=-1), dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
