{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb213fd74d14c78a879695961fbe6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/csgrad/sunilruf/'\n",
    "path = \"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/rank_64/rank_64_2/checkpoint-50\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0d804df5594da5a8f2dd54ff7a6b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"thu-coai/esconv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 910\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 195\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 195\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(dataset['test'][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hai. My name is Tom and i really need your help.\n"
     ]
    }
   ],
   "source": [
    "print(data['dialog'][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(data['dialog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": data['dialog'][1]['text']\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity.compute(model_id=\"clm-gpu/checkpoint-138000\",\n",
    "                             add_start_token=False,\n",
    "                             predictions=input_texts, \n",
    "                             local_file_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "  0%|          | 0/8 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 32000])\n",
      "tensor([[[-20.6981, -20.6551, -15.5066,  ..., -19.3895, -18.4305, -19.0555],\n",
      "         [-18.3835, -17.8367, -13.7156,  ..., -14.1101, -15.0202, -13.6492],\n",
      "         [-16.3907, -15.9679, -22.1857,  ..., -15.3744, -18.0782, -14.9067],\n",
      "         ...,\n",
      "         [-18.1543, -17.1757, -25.7851,  ..., -18.1718, -22.5146, -22.0420],\n",
      "         [-17.8572, -15.6346, -22.2029,  ..., -19.1551, -23.3944, -23.0076],\n",
      "         [-23.2699, -22.2592, -24.8754,  ..., -25.1683, -24.6605, -25.1176]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "1\n",
      "tensor(21.2249, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 32000])\n",
      "tensor([[[-20.7354, -20.6925, -15.5313,  ..., -19.4229, -18.4678, -19.0909],\n",
      "         [-18.3172, -17.7508, -13.7078,  ..., -14.0926, -15.0243, -13.6746],\n",
      "         [-16.5105, -16.0730, -22.1863,  ..., -15.5403, -18.2195, -15.0703],\n",
      "         ...,\n",
      "         [-17.6761, -16.6546, -15.3109,  ..., -17.0941, -23.8577, -23.2601],\n",
      "         [-18.5505, -16.3640, -10.5261,  ..., -19.9962, -22.3171, -21.1540],\n",
      "         [-19.5804, -17.8523,  -6.7074,  ..., -22.5394, -19.7914, -20.2269]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "2\n",
      "tensor(21.4581, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 113, 32000])\n",
      "tensor([[[-20.7414, -20.6985, -15.5384,  ..., -19.4328, -18.4719, -19.0949],\n",
      "         [-18.3216, -17.7591, -13.6693,  ..., -14.1068, -14.9993, -13.6595],\n",
      "         [-16.5549, -16.1194, -22.2073,  ..., -15.5969, -18.2639, -15.0990],\n",
      "         ...,\n",
      "         [-18.0557, -16.8584, -16.6299,  ..., -18.5420, -24.6318, -23.1299],\n",
      "         [-16.7180, -14.6682, -11.3010,  ..., -18.5040, -20.8635, -20.5061],\n",
      "         [-15.6895, -13.5562,  -1.9293,  ..., -18.0660, -18.7750, -17.8004]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "3\n",
      "tensor(20.3025, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 126, 32000])\n",
      "tensor([[[-20.7234, -20.6804, -15.5279,  ..., -19.4148, -18.4577, -19.0788],\n",
      "         [-18.3300, -17.7714, -13.6972,  ..., -14.1054, -15.0214, -13.6698],\n",
      "         [-16.5417, -16.1159, -22.3142,  ..., -15.5883, -18.2732, -15.0841],\n",
      "         ...,\n",
      "         [-19.4738, -18.2765, -17.2648,  ..., -19.1027, -25.6964, -24.0793],\n",
      "         [-17.9947, -15.8810, -10.8165,  ..., -19.1056, -20.9142, -20.3321],\n",
      "         [-16.5828, -14.4276,  -1.0018,  ..., -18.8592, -18.3983, -17.3963]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "4\n",
      "tensor(20.0774, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 32000])\n",
      "tensor([[[-20.7276, -20.6885, -15.5298,  ..., -19.4229, -18.4658, -19.0869],\n",
      "         [-18.3257, -17.7593, -13.6851,  ..., -14.1069, -15.0171, -13.6675],\n",
      "         [-16.4675, -16.0700, -22.1891,  ..., -15.4450, -18.1071, -15.0323],\n",
      "         ...,\n",
      "         [-19.9516, -18.6508, -16.0199,  ..., -19.3403, -25.7270, -24.8461],\n",
      "         [-17.3110, -15.6657, -10.2009,  ..., -18.1794, -19.3903, -18.9431],\n",
      "         [-18.1945, -16.1008,  -1.5128,  ..., -19.6808, -18.5460, -18.6202]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "5\n",
      "tensor(19.6853, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 173, 32000])\n",
      "tensor([[[-20.7316, -20.6847, -15.5311,  ..., -19.4230, -18.4640, -19.0890],\n",
      "         [-18.3091, -17.7427, -13.6919,  ..., -14.0767, -15.0005, -13.6509],\n",
      "         [-16.5237, -16.0842, -22.1789,  ..., -15.5456, -18.2551, -15.0574],\n",
      "         ...,\n",
      "         [-19.7417, -18.3013, -15.1333,  ..., -19.9908, -25.2427, -24.4419],\n",
      "         [-15.8280, -13.9081,  -9.6835,  ..., -17.3804, -19.0760, -18.5643],\n",
      "         [-16.0667, -14.0130,  -1.3343,  ..., -15.4593, -16.8792, -17.0257]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "6\n",
      "tensor(19.4162, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 209, 32000])\n",
      "tensor([[[-20.7260, -20.6869, -15.5296,  ..., -19.4213, -18.4623, -19.0854],\n",
      "         [-18.3533, -17.7947, -13.6873,  ..., -14.1229, -15.0213, -13.6854],\n",
      "         [-16.4938, -16.0739, -22.2018,  ..., -15.4391, -18.1959, -15.0634],\n",
      "         ...,\n",
      "         [-20.9220, -19.7033, -15.5588,  ..., -20.5275, -26.4943, -25.7404],\n",
      "         [-18.0833, -16.3580, -11.1920,  ..., -19.2725, -20.3961, -19.2652],\n",
      "         [-18.0289, -15.9321,  -0.1334,  ..., -18.8755, -19.0201, -18.6783]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "7\n",
      "tensor(18.9883, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 32000])\n",
      "tensor([[[-20.7304, -20.6874, -15.5348,  ..., -19.4257, -18.4667, -19.0897],\n",
      "         [-18.3111, -17.7486, -13.6861,  ..., -14.0923, -14.9966, -13.6607],\n",
      "         [-16.4590, -16.0196, -22.1778,  ..., -15.5061, -18.1504, -15.0367],\n",
      "         ...,\n",
      "         [-21.0392, -19.8634, -16.9377,  ..., -21.0431, -26.5099, -25.3146],\n",
      "         [-17.0679, -15.3189, -10.6705,  ..., -18.6661, -19.7955, -19.2813],\n",
      "         [-17.2674, -15.3116,  -0.1688,  ..., -17.6649, -20.2586, -18.5184]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "8\n",
      "tensor(18.7629, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 243, 32000])\n",
      "tensor([[[-20.7308, -20.6879, -15.5309,  ..., -19.4222, -18.4672, -19.0863],\n",
      "         [-18.3336, -17.7711, -13.6910,  ..., -14.1129, -15.0367, -13.6715],\n",
      "         [-16.4447, -16.0087, -22.1957,  ..., -15.5019, -18.1781, -15.0256],\n",
      "         ...,\n",
      "         [-21.7911, -20.7052, -18.8868,  ..., -21.6309, -27.0411, -26.3302],\n",
      "         [-17.4192, -15.4612, -12.0335,  ..., -19.3428, -20.5520, -19.8142],\n",
      "         [-16.4691, -14.4379,  -0.4223,  ..., -17.2973, -20.0121, -18.0316]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "9\n",
      "tensor(18.5271, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 32000])\n",
      "tensor([[[-20.7315, -20.6885, -15.5292,  ..., -19.4268, -18.4659, -19.0870],\n",
      "         [-18.2986, -17.7322, -13.6990,  ..., -14.0994, -15.0369, -13.6756],\n",
      "         [-16.5207, -16.0891, -22.2229,  ..., -15.5317, -18.2639, -15.1621],\n",
      "         ...,\n",
      "         [-21.1388, -20.1310, -19.4337,  ..., -21.2032, -26.5685, -25.7071],\n",
      "         [-17.6306, -16.0725, -11.8928,  ..., -19.6243, -20.1028, -19.4944],\n",
      "         [-16.7585, -14.7214,  -0.5427,  ..., -18.0779, -19.9763, -18.5095]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "10\n",
      "tensor(18.2304, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 269, 32000])\n",
      "tensor([[[-20.7055, -20.6625, -15.5141,  ..., -19.4008, -18.4418, -19.0648],\n",
      "         [-18.3375, -17.7671, -13.6949,  ..., -14.1031, -15.0425, -13.6793],\n",
      "         [-16.4920, -16.0789, -22.1727,  ..., -15.4273, -18.1297, -15.0406],\n",
      "         ...,\n",
      "         [-21.0870, -19.8185, -16.6759,  ..., -21.3654, -25.9650, -25.4493],\n",
      "         [-16.9007, -15.0345, -11.0052,  ..., -19.9124, -20.0540, -19.1370],\n",
      "         [-17.0507, -14.7397,  -0.5859,  ..., -18.7070, -19.4140, -17.4238]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "11\n",
      "tensor(17.8940, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 280, 32000])\n",
      "tensor([[[-20.7459, -20.7029, -15.5442,  ..., -19.4373, -18.4803, -19.1033],\n",
      "         [-18.3042, -17.7417, -13.7104,  ..., -14.0815, -15.0307, -13.6616],\n",
      "         [-16.5492, -16.1234, -22.2318,  ..., -15.5150, -18.2298, -15.1518],\n",
      "         ...,\n",
      "         [-20.7989, -19.5099, -17.5157,  ..., -21.2843, -26.3556, -25.5528],\n",
      "         [-16.9544, -14.9622, -10.9759,  ..., -19.7991, -20.0520, -19.3665],\n",
      "         [-18.5403, -16.3812,  -0.1321,  ..., -19.6439, -20.1399, -19.4368]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "12\n",
      "tensor(17.5845, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 299, 32000])\n",
      "tensor([[[-20.7530, -20.7139, -15.5549,  ..., -19.4483, -18.4835, -19.1065],\n",
      "         [-18.3657, -17.8188, -13.7700,  ..., -14.1235, -14.9888, -13.6431],\n",
      "         [-16.3264, -15.9133, -22.2726,  ..., -15.3732, -18.0070, -14.8273],\n",
      "         ...,\n",
      "         [-21.6113, -20.3283, -14.8908,  ..., -21.2853, -26.2814, -25.7209],\n",
      "         [-17.2927, -15.5227,  -8.8479,  ..., -18.6447, -18.7873, -18.4289],\n",
      "         [-17.5598, -15.7029,  -0.3059,  ..., -18.2141, -18.6887, -18.4953]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "13\n",
      "tensor(17.4045, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 310, 32000])\n",
      "tensor([[[-20.7518, -20.7128, -15.5566,  ..., -19.4393, -18.4764, -19.0995],\n",
      "         [-18.3700, -17.8309, -13.7997,  ..., -14.1473, -14.9833, -13.6649],\n",
      "         [-16.3521, -15.9742, -22.4312,  ..., -15.3616, -17.9703, -14.8738],\n",
      "         ...,\n",
      "         [-19.7226, -18.1335, -12.8093,  ..., -19.4724, -23.8679, -23.0593],\n",
      "         [-17.3994, -15.3698,  -7.3102,  ..., -18.9303, -19.5504, -18.8073],\n",
      "         [-18.7913, -16.8792,  -0.0374,  ..., -19.3284, -19.9241, -19.2894]]],\n",
      "       device='cuda:3')\n",
      "torch.Size([])\n",
      "14\n",
      "tensor(17.2012, device='cuda:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 1024\n",
    "stride = 128\n",
    "\n",
    "#encodings = tokenizer.encode_plus(data['dialog'][1]['text'], return_tensors=\"pt\").to('cuda')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "seq_len = 1024\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "messages = []\n",
    "i = 1\n",
    "while i<29:\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        messages.append({\"role\": \"user\", \"content\":  data['dialog'][i]['text']})\n",
    "        i+=2\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        #input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "        input_ids = tokenizer.apply_chat_template(messages, tokenize=True, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                labels=target_ids,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "        )\n",
    "            val = (tokenizer.decode(outputs[0], skip_special_tokens=True)[0].split(\"[/INST]\")[-1])\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            print(outputs.logits.shape)\n",
    "            print(torch.log(torch.softmax(outputs.logits,dim=-1)))\n",
    "            #decoded_output = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs.logits.argmax(-1)]\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            #print(outputs)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            print(neg_log_likelihood.shape)\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        print(len(nlls))\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    print(ppl)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(455.2411, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(333.4897, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(359.4648, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(452.8662, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(541.8233, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(641.3824, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(701.5912, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(781.6432, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(872.0746, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(965.7397, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1066.2126, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1179.7234, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1296.0780, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1420.3734, device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 1024\n",
    "stride = 128\n",
    "\n",
    "#encodings = tokenizer.encode_plus(data['dialog'][1]['text'], return_tensors=\"pt\").to('cuda')\n",
    "tokenizer1.pad_token = tokenizer1.eos_token\n",
    "seq_len = 1024\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "messages = []\n",
    "i = 1\n",
    "while i<29:\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        messages.append({\"role\": \"user\", \"content\":  data['dialog'][i]['text']})\n",
    "        i+=2\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        #input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "        input_ids = tokenizer1.apply_chat_template(messages, tokenize=True, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = model1.generate(\n",
    "                input_ids=input_ids,\n",
    "                labels=target_ids,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "        )\n",
    "            val = (tokenizer1.decode(outputs[0], skip_special_tokens=True)[0].split(\"[/INST]\")[-1])\n",
    "            outputs = model1(input_ids, labels=target_ids)\n",
    "            #decoded_output = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs.logits.argmax(-1)]\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            #print(outputs)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    print(ppl)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "#max_length = model.config.n_positions\n",
    "stride = 512\n",
    "text = \"Absolutely, building this relationship means being there for each other through thick and thin. Whether it's sharing our joys, navigating challenges, or simply lending an ear, I'm dedicated to fostering a connection grounded in trust, empathy, and genuine care. Together, we can create a safe space where open dialogue and mutual support flourish.\"\n",
    "encodings = tokenizer.encode_plus(\"Of course! I'm here to help and chat with you whenever you need.\", return_tensors=\"pt\").to('cuda')\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6309, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.mean(torch.max(torch.softmax(outputs.logits, dim=-1), dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([[0.4461, 0.4940, 0.7285, 0.4279, 0.1381, 0.9768, 0.9906, 0.7321, 0.3723]],\n",
      "       device='cuda:1'),\n",
      "indices=tensor([[22478,   264, 10360,   354,   272,   266, 28127,  7768,    13]],\n",
      "       device='cuda:1'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.max(torch.softmax(outputs.logits, dim=-1),dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6380, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.mean(torch.max(torch.softmax(outputs.logits, dim=-1), dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143.4096     31.732403   16.633274   11.515069    9.100802    7.727912\n",
      "    6.8666377   6.260498 ]\n",
      " [ 29.021667   15.979132   10.856503    8.518468    7.2484775   6.444537\n",
      "    5.8987575   5.5092688]\n",
      " [109.79915    33.625156   18.313675   12.627146    9.882281    8.280714\n",
      "    7.2584205   6.5607643]\n",
      " [ 29.021667   16.490791   11.03499     8.539032    7.284541    6.5211115\n",
      "    6.0132794   5.627395 ]\n",
      " [ 86.880264   33.24064    18.463526   13.013455   10.339102    8.805614\n",
      "    7.8480744   7.169255 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/instruct-mistral/evaluate/perplexity_scores/llama/llama_base.npy\", allow_pickle=True)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
