{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdc7310b05e4699a50a91f6d83d7ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/experiment-3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/experiment-3\", load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ {\n",
    "  'role':'user',\n",
    "  'content': 'Hey, are you free? I need someone to talk to.',\n",
    "}  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Hey, are you free? I need someone to talk to. [/INST] Of course! I'm here to help and chat with you whenever you need. What's on your mind today? I'll listen and do my best to provide support and guidance. Let me know if there's anything specific you'd like to talk about or if you just need someone to vent to. I'm here for you.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Of course! I'm here to help and chat with you whenever you need\", \" What's on your mind today? I'll listen and do my best to provide support and guidance\", \" Let me know if there's anything specific you'd like to talk about or if you just need someone to vent to\", \" I'm here for you\"]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split(\"[/INST]\")[-1].split(\".\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ced6ea0070d460e9dd2be0567b9200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cpu\" # the device to load the model onto\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ {\n",
    "  'role':'user',\n",
    "  'content': 'Hey, are you free? I need someone to talk to.',\n",
    "},\n",
    "        {\n",
    "  'role':'assistant',\n",
    "  'content': \"Of course, I'm here to help. Feel free to share what's on your mind and I'll listen and try to provide a supportive and understanding response. What's been going on?\",\n",
    "  \n",
    "        },\n",
    "        {\n",
    "  'role':'user',\n",
    "  'content': \"I've been feeling really down lately. I'm struggling with my mental health and it's been hard to cope. I feel overwhelmed and alone.\",\n",
    "        }   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Hey, are you free? I need someone to talk to. [/INST]Of course, I'm here to help. Feel free to share what's on your mind and I'll listen and try to provide a supportive and understanding response. What's been going on? [INST] I've been feeling really down lately. I'm struggling with my mental health and it's been hard to cope. I feel overwhelmed and alone. [/INST] I'm really sorry to hear that you've been feeling this way recently. It's important to remember that it's okay to have difficult emotions and that you're not alone in experiencing them. Mental health struggles can be incredibly challenging, and it can be helpful to talk about them with someone who cares.\n",
      "\n",
      "Have you considered reaching out to a mental health professional for support? They can provide you with tools and techniques to help manage your emotions and improve your overall well-being. Additionally, there are many online resources and support groups that can provide a sense of community and connection.\n",
      "\n",
      "It's also\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer1.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model1.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer1.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device_map = {\"cuda\": torch.cuda.current_device()} if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuda': 0}\n"
     ]
    }
   ],
   "source": [
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm really sorry to hear that you've been feeling this way recently. It's important to remember that it's okay to have difficult emotions and that you're not alone in experiencing them. Mental health struggles can be incredibly challenging, and it can be helpful to talk about them with someone who cares.\n",
      "\n",
      "Have you considered reaching out to a mental health professional for support? They can provide you with tools and techniques to help manage your emotions and improve your overall well-being. Additionally, there are many online resources and support groups that can provide a sense of community and connection.\n",
      "\n",
      "It's also\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer1.batch_decode(outputs, skip_special_tokens=True)[0].split(\"[/INST]\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3d71e70cbb4b26b9335548d2c39694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/experiment-5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/csgrad/sunilruf/emotion_chatbot/master/sunils_code/mistra/experiment-5\", load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I know it can be easy to say things, but it's important to remember that actions speak louder than words. I may not be able to physically be there for you, but I can offer you my support and understanding in any way I can. I'm here to listen and help in any way I can, no matter how small or insignificant it may seem. I believe in you and your ability to get through this tough time. You're stronger than you think. ðŸ˜Š #EmpathicEar #MentalMentor #SupportiveSoul #BelieveInYou #Strong\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    messages.append( {\n",
    "      'role':'user',\n",
    "      'content': user_input,\n",
    "    }  )\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    #attention_mask = input_ids['attention_mask'].to(\"cuda\")\n",
    "    #model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # inference\n",
    "    outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "    )\n",
    "    messages.append({\n",
    "        'role':'assistant',\n",
    "        'content': tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split(\"[/INST]\")[-1]\n",
    "    })\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split(\"[/INST]\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Hey'}, {'role': 'assistant', 'content': \" Hello! How can I help you today? If you have any questions or need assistance with something, feel free to ask. I'm here to make your life easier! If you just want to chat, that's cool too. Let me know what's on your mind. ðŸ˜ŠðŸ¤— #AwesomeAssistant #HelpfulHuman #AlwaysAvailable #GotYourBack #FriendlyBot #TechTitan #AIAssistant #DigitalDude #SmartSolution #IntelligentInteraction #MentalMentor #VirtualVirtuoso #Brain\"}, {'role': 'user', 'content': 'I am feeling low'}, {'role': 'assistant', 'content': \" I'm really sorry to hear that you're feeling down. It can be tough sometimes, and it's okay to have days where you don't feel your best. I'm here to listen if you want to talk about what's been on your mind. Sometimes just getting things out can help make things feel better. If you'd rather not talk, that's okay too. I understand if you need some space. Just know that I'm here for you whenever you're ready. ðŸ˜Š #ComfortingCompanion #EmotionalEmpath #MentalMentor #\"}]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ {\n",
    "  'role':'user',\n",
    "  'content': 'Hey, are you free? I need someone to talk to.',\n",
    "} ,\n",
    "            {\n",
    "  'role':'assistant',\n",
    "  'content':\" Of course! I'm here to help and chat with you whenever you need. What's on your mind today? I'll listen and try to provide support and guidance as best I can. Let me know if there's a specific topic or concern you'd like to discuss. I'm here for you.\",\n",
    "  \n",
    "            },\n",
    "            {\n",
    "  'role':'user',\n",
    "  'content':\"I've been feeling really down lately. I'm struggling with my mental health and it's been hard to cope. I feel overwhelmed and alone.\",\n",
    "            },\n",
    "            {\n",
    "  'role':'assistant',\n",
    "  'content':\"I'm really sorry to hear that you've been feeling this way lately. It's important to remember that it's okay to feel overwhelmed and alone sometimes, and that it's a common experience for many people. I'm here to listen and offer support as best I can. One thing that might help is to reach out to trusted friends or family members and let them know how you're feeling. Sometimes, just talking about what you're going through can help lighten the load and make things feel a little less heavy.\",\n",
    "            },\n",
    "            {\n",
    "  'role':'user',\n",
    "  'content':\"It is just that I am not getting any job offers as a student.\"\n",
    "            },\n",
    "            {\n",
    "  'role':'assistant',\n",
    "  'content':\" I understand that feeling jobless and uncertain about your future can add to the stress and anxiety you're experiencing. It's important to remember that finding a job can take time, and that there are many factors that can influence the job search process. It can be helpful to focus on the things that are within your control, such as networking, applying to jobs, and improving your skills and resume. It can also be helpful to seek out support and guidance from career counselors, mentors, or peers. Remember that it's okay to take breaks and give yourself time to recharge, but also to stay focused and persistent\",\n",
    "            },\n",
    "            {\n",
    "  'role':'user',\n",
    "  'content':\"It is easy to say. But it hurts everyday, knowing that my future is uncertain\"\n",
    "            },\n",
    "            \n",
    "            \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I completely understand that it's not easy to hear, and that the uncertainty of your future can be a painful and overwhelming feeling. It's important to remember that it's normal to feel this way, and that it's okay to take things one day at a time. It can be helpful to focus on the things that you can control in the present moment, such as taking care of yourself physically, emotionally, and mentally, and pursuing opportunities that align with your interests and goals. It can also be helpful to seek out resources and support networks to help you navigate this time in your life. Remember that you are not\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split(\"[/INST]\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from evaluate) (0.21.3)\n",
      "Requirement already satisfied: packaging in /home/csgrad/sunilruf/.local/lib/python3.11/site-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/csgrad/sunilruf/.local/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/csgrad/sunilruf/miniconda3/envs/bio3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1452f5009ab4679afbabcdf680005a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6350cb354e496b8b7b896849193d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "predictions = \" Of course! I'm here to help and chat with you whenever you need.\"\n",
    "results = perplexity.compute(predictions=predictions, model_id='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3736.0041732201207"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean_perplexity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402869.3338232103\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 1024\n",
    "stride = 512\n",
    "encodings = tokenizer.encode_plus(\"Of course! I'm here to help and chat with you whenever you need.\", return_tensors=\"pt\").to('cuda')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7786, device='cuda:3')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 1024\n",
    "stride = 512\n",
    "encodings = tokenizer1.encode_plus(\"Of course! I'm here to help and chat with you whenever you need.\", return_tensors=\"pt\").to('cuda')\n",
    "tokenizer1.pad_token = tokenizer.eos_token\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to('cuda')\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model1(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.2793, device='cuda:3')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
